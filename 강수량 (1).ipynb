{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, BatchNormalization, concatenate, Input, ConvLSTM2D, Reshape, Conv3D, Flatten, LSTM, GRU, Dense,Dropout, Add\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(120,120), n_channels=1, n_timesteps = 4, shuffle=True, augment_data = True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_timesteps = n_timesteps \n",
    "        self.shuffle = shuffle\n",
    "        self.augment_data = augment_data \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        \n",
    "        if self.augment_data == True:  # only augment data when training \n",
    "            # Initialization\n",
    "            X = np.empty((self.batch_size*5, 120, 120, 4))\n",
    "            y = np.empty((self.batch_size*5, 120, 120, 1)) \n",
    "\n",
    "            # Generate data\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                data = np.load('./storage/precipitation/train/' + ID)\n",
    "                # Store sample\n",
    "                x_data = data[:,:,:4] \n",
    "                y_data = data[:,:,-1].reshape((120,120,1)) \n",
    "                \n",
    "                X[i,] = x_data\n",
    "                y[i] = y_data \n",
    "                \n",
    "                # add 90 degrees rotation \n",
    "                X[i+self.batch_size,] = np.rot90(x_data)\n",
    "                y[i+self.batch_size] = np.rot90(y_data)  \n",
    "                \n",
    "                # add 180 degrees rotation \n",
    "                X[i+self.batch_size*2,] = np.rot90(np.rot90(x_data)) \n",
    "                X[i+self.batch_size*2] = np.rot90(np.rot90(y_data))\n",
    "                        \n",
    "                # add horizontal flip \n",
    "                X[i+self.batch_size*3,] = np.fliplr(x_data)\n",
    "                y[i+self.batch_size*3] = np.fliplr(y_data) \n",
    "                \n",
    "                # add vertical filp \n",
    "                X[i+self.batch_size*4,] = np.flipud(x_data) \n",
    "                y[i+self.batch_size*4] = np.flipud(y_data)\n",
    "            \n",
    "            # shuffle once more to make training harder \n",
    "            X,y = shuffle(X,y) \n",
    "            return (X, y)\n",
    "        \n",
    "        else: \n",
    "            # Initialization\n",
    "            X = np.empty((self.batch_size, 120, 120, 4))\n",
    "            y = np.empty((self.batch_size, 120, 120, 1)) \n",
    "\n",
    "            # Generate data\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                data = np.load('./storage/precipitation/train/' + ID)\n",
    "                # Store sample\n",
    "                X[i,] = data[:,:,:4] \n",
    "                y[i] = data[:,:,-1].reshape((120,120,1)) \n",
    "            \n",
    "            return (X, y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses skip connections and also adds information from both MaxPooling2D and AveragePooling2D \n",
    "def conv2d_block(input_layer, n_filters, kernel):\n",
    "    conv1 = Conv2D(n_filters, kernel, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_layer)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv2 = Conv2D(n_filters, kernel, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1) \n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv3 = Conv2D(n_filters, kernel, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2) \n",
    "    conv3 = Add()([conv3, conv1])   \n",
    "    conv3 = BatchNormalization()(conv3) \n",
    "    maxpool = MaxPooling2D((2,2))(conv3) \n",
    "    avgpool = AveragePooling2D((2,2))(conv3)\n",
    "    ret = Add()([maxpool,avgpool]) \n",
    "    return conv3, ret # conv3: for UNet concatenation, ret: to pass into succeeding layers \n",
    "\n",
    "def uconv2d_block(input_layer, n_filters, kernel): \n",
    "    conv1 = Conv2D(n_filters, kernel, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_layer)\n",
    "    conv1 = BatchNormalization()(conv1) \n",
    "    conv2 = Conv2D(n_filters, kernel, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1) \n",
    "    conv2 = BatchNormalization()(conv2) \n",
    "    conv3 = Conv2D(n_filters, kernel, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2) \n",
    "    conv3 = Add()([conv3,conv1]) \n",
    "    conv3 = BatchNormalization()(conv3) \n",
    "    return conv3 \n",
    "\n",
    "# this version takes too long to train \n",
    "def build_model(start_neurons):\n",
    "    \n",
    "    inputs = Input((120,120,4)) \n",
    "    \n",
    "    bn = BatchNormalization()(inputs)\n",
    "    \n",
    "    # compressing stages - skip connections with multiple kernel sizes.  \n",
    "    conv11, pooled11 = conv2d_block(bn, start_neurons, 1)\n",
    "    conv21, pooled21 = conv2d_block(bn, start_neurons, 3) \n",
    "    conv31, pooled31 = conv2d_block(bn, start_neurons, 5) \n",
    "    conv1 = concatenate([conv11,conv21,conv31]) \n",
    "    pooled1 = concatenate([pooled11,pooled21,pooled21])\n",
    "    \n",
    "    \n",
    "    conv12, pooled12 = conv2d_block(pooled1, start_neurons * 2, 1)\n",
    "    conv22, pooled22 = conv2d_block(pooled1, start_neurons * 2, 3) \n",
    "    conv32, pooled32 = conv2d_block(pooled1, start_neurons * 2, 5) \n",
    "    conv2 = concatenate([conv12,conv22,conv32]) \n",
    "    pooled2 = concatenate([pooled12,pooled22,pooled32]) \n",
    "    \n",
    "    conv13, pooled13 = conv2d_block(pooled2, start_neurons * 4, 1) \n",
    "    conv23, pooled23 = conv2d_block(pooled2, start_neurons * 4, 3) \n",
    "    conv33, pooled33 = conv2d_block(pooled2, start_neurons * 4, 5) \n",
    "    conv3 = concatenate([conv13,conv23,conv33]) \n",
    "    pooled3 = concatenate([pooled13,pooled23,pooled33])  \n",
    "\n",
    "    # middle convolutional layer, keeping it simple \n",
    "    convm = Conv2D(start_neurons * 8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pooled3)\n",
    "    \n",
    "    # decompressing stage - skip connections with multiple kernel sizes \n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, 3, strides = 2, padding = 'same')(convm)\n",
    "    uconv3 = concatenate([deconv3,conv3])\n",
    "    uconv13 = uconv2d_block(uconv3, start_neurons * 4, 1)\n",
    "    uconv23 = uconv2d_block(uconv3, start_neurons * 4, 3) \n",
    "    uconv33 = uconv2d_block(uconv3, start_neurons * 4, 5)\n",
    "    uconv3 = concatenate([uconv13,uconv23,uconv33]) \n",
    "    \n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, 3, strides = 2, padding = 'same')(uconv3) \n",
    "    uconv2 = concatenate([deconv2,conv2]) \n",
    "    uconv12 = uconv2d_block(uconv2, start_neurons * 2, 1) \n",
    "    uconv22 = uconv2d_block(uconv2, start_neurons * 2, 3)\n",
    "    uconv32 = uconv2d_block(uconv2, start_neurons * 2, 5) \n",
    "    uconv2 = concatenate([uconv12,uconv22,uconv32]) \n",
    "    \n",
    "    deconv1 = Conv2DTranspose(start_neurons, 3, strides = 2, padding = 'same')(uconv2) \n",
    "    uconv1 = concatenate([deconv1,conv1]) \n",
    "    uconv11 = uconv2d_block(uconv1, start_neurons, 1)\n",
    "    uconv21 = uconv2d_block(uconv1, start_neurons, 3) \n",
    "    uconv31 = uconv2d_block(uconv1, start_neurons, 5) \n",
    "\n",
    "    outputs = Conv2D(1, (1,1), padding=\"same\", activation='relu')(uconv1)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mae',optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_2(start_neurons):\n",
    "    \n",
    "    inputs = Input((120,120,4)) \n",
    "    \n",
    "    bn = BatchNormalization()(inputs)\n",
    "    \n",
    "    # compressing stages - skip connections with multiple kernel sizes.  \n",
    "    conv1, pooled1 = conv2d_block(bn, start_neurons, 3) \n",
    "    conv2, pooled2 = conv2d_block(pooled1, start_neurons * 2, 3)\n",
    "    conv3, pooled3 = conv2d_block(pooled2, start_neurons * 4, 3) \n",
    "\n",
    "    # middle convolutional layer, keeping it simple \n",
    "    convm = Conv2D(start_neurons * 8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pooled3)\n",
    "    \n",
    "    # decompressing stage - skip connections with multiple kernel sizes \n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, 3, strides = 2, padding = 'same')(convm)\n",
    "    uconv3 = concatenate([deconv3,conv3])\n",
    "    uconv3 = uconv2d_block(uconv3, start_neurons * 4, 3) \n",
    "    \n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, 3, strides = 2, padding = 'same')(uconv3) \n",
    "    uconv2 = concatenate([deconv2,conv2])\n",
    "    uconv2 = uconv2d_block(uconv2, start_neurons * 2, 3)\n",
    "    \n",
    "    deconv1 = Conv2DTranspose(start_neurons, 3, strides = 2, padding = 'same')(uconv2) \n",
    "    uconv1 = concatenate([deconv1,conv1]) \n",
    "    uconv1 = uconv2d_block(uconv1, start_neurons, 3) \n",
    "\n",
    "    outputs = Conv2D(1, (1,1), padding=\"same\", activation='relu')(uconv1)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mae',optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_3(start_neurons):\n",
    "    \n",
    "    inputs = Input((120,120,4)) \n",
    "    \n",
    "    bn = BatchNormalization()(inputs)\n",
    "    \n",
    "    # compressing stages - skip connections with multiple kernel sizes.  \n",
    "    conv1, pooled1 = conv2d_block(bn, start_neurons, 3) \n",
    "    conv2, pooled2 = conv2d_block(pooled1, start_neurons * 2, 3)\n",
    "\n",
    "    # middle convolutional layer, keeping it simple \n",
    "    convm = Conv2D(start_neurons * 4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pooled2)\n",
    "    \n",
    "    # decompressing stage - skip connections with multiple kernel sizes     \n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, 3, strides = 2, padding = 'same')(convm) \n",
    "    uconv2 = concatenate([deconv2,conv2])\n",
    "    uconv2 = uconv2d_block(uconv2, start_neurons * 2, 3)\n",
    "    \n",
    "    deconv1 = Conv2DTranspose(start_neurons, 3, strides = 2, padding = 'same')(uconv2) \n",
    "    uconv1 = concatenate([deconv1,conv1]) \n",
    "    uconv1 = uconv2d_block(uconv1, start_neurons, 3) \n",
    "\n",
    "    outputs = Conv2D(1, (1,1), padding=\"same\", activation='relu')(uconv1)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mae',optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def build_resnet(middle_depth):\n",
    "    \n",
    "    inputs = Input((120,120,4)) \n",
    "    bn = BatchNormalization()(inputs)\n",
    "    conv0 = Conv2D(32, kernel_size=1, strides=1, padding='same', activation='relu')(bn)\n",
    "\n",
    "    bn = BatchNormalization()(conv0)\n",
    "    conv = Conv2D(32, kernel_size=2, strides=1, padding='same', activation='relu')(bn)\n",
    "    concat = concatenate([conv0, conv], axis=3)\n",
    "\n",
    "    bn = BatchNormalization()(concat)\n",
    "    conv = Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')(bn)\n",
    "    concat = concatenate([concat, conv], axis=3)\n",
    "\n",
    "    for i in range(middle_depth):\n",
    "        bn = BatchNormalization()(concat)\n",
    "        conv = Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')(bn)\n",
    "        concat = concatenate([concat, conv], axis=3)\n",
    "\n",
    "    bn = BatchNormalization()(concat)\n",
    "    outputs = Conv2D(1, kernel_size=1, strides=1, padding='same', activation='relu')(bn)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def base_UNET(start_neurons):\n",
    "    \n",
    "    inputs = Input((120,120,4))\n",
    "    bn = BatchNormalization()(inputs)\n",
    "    \n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(bn)\n",
    "    pool1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(pool1)\n",
    "\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    pool2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(pool2)\n",
    "\n",
    "    convm = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = BatchNormalization()(uconv1)\n",
    "    outputs = Conv2D(1, (1,1), padding=\"same\", activation='relu')(uconv1) \n",
    "    \n",
    "    model = Model(inputs =inputs, outputs=outputs) \n",
    "    model.compile(loss='mae',optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def base_UNET2(start_neurons): \n",
    "    inputs = Input((120,120,4))\n",
    "    bn = BatchNormalization()(inputs)\n",
    "    \n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(bn)\n",
    "    pool1 = BatchNormalization()(conv1)\n",
    "    maxpool1 = MaxPooling2D((2, 2))(pool1) \n",
    "    avgpool1 = AveragePooling2D((2,2))(pool1) \n",
    "    pool1 = Add()([maxpool1,avgpool1]) \n",
    "\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    pool2 = BatchNormalization()(conv2)\n",
    "    maxpool2 = MaxPooling2D((2, 2))(pool2) \n",
    "    avgpool2 = AveragePooling2D((2,2))(pool2) \n",
    "    pool2 = Add()([maxpool2,avgpool2]) \n",
    "\n",
    "    convm = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = BatchNormalization()(uconv1)\n",
    "    outputs = Conv2D(1, (1,1), padding=\"same\", activation='relu')(uconv1) \n",
    "    \n",
    "    model = Model(inputs =inputs, outputs=outputs) \n",
    "    model.compile(loss='mae',optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_UNET3(start_neurons): \n",
    "    inputs = Input((120,120,4))\n",
    "    bn = BatchNormalization()(inputs)\n",
    "    \n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(bn) \n",
    "    conv12 = Conv2D(start_neurons * 1, 5, activation = 'relu', padding = 'same')(bn) \n",
    "    conv1 = concatenate([conv1,conv12]) \n",
    "    pool1 = BatchNormalization()(conv1)\n",
    "    maxpool1 = MaxPooling2D((2, 2))(pool1) \n",
    "    avgpool1 = AveragePooling2D((2,2))(pool1) \n",
    "    pool1 = Add()([maxpool1,avgpool1]) \n",
    "\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    pool2 = BatchNormalization()(conv2)\n",
    "    maxpool2 = MaxPooling2D((2, 2))(pool2) \n",
    "    avgpool2 = AveragePooling2D((2,2))(pool2) \n",
    "    pool2 = Add()([maxpool2,avgpool2]) \n",
    "\n",
    "    convm = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = BatchNormalization()(uconv1)\n",
    "    outputs = Conv2D(1, (1,1), padding=\"same\", activation='relu')(uconv1) \n",
    "    \n",
    "    model = Model(inputs =inputs, outputs=outputs) \n",
    "    model.compile(loss='mae',optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ Fold 1 Training ........\n",
      "Epoch 1/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.9419\n",
      "Epoch 00001: val_loss improved from inf to 3.13009, saving model to ./storage/precip_unet_base/kfold1/epoch_001_val_loss_3.130.h5\n",
      "1568/1568 [==============================] - 1247s 796ms/step - loss: 5.9411 - val_loss: 3.1301\n",
      "Epoch 2/100\n",
      " 837/1568 [===============>..............] - ETA: 9:24 - loss: 5.0915"
     ]
    }
   ],
   "source": [
    "train_files = [x for x in os.listdir('./storage/precipitation/train/')] \n",
    "\n",
    "def k_fold(k,files):  \n",
    "    folds = [] \n",
    "    fold_size = len(files) // k \n",
    "    for i in range(k): \n",
    "        if i == k-1:  \n",
    "            l = files[i*fold_size:] \n",
    "        else: \n",
    "            l = files[i*fold_size:(i+1)*fold_size]  \n",
    "        folds.append(l)   \n",
    "    return folds  \n",
    "\n",
    "train_files = shuffle(train_files) # shuffle train files before splitting them into a fold \n",
    "train_folds = k_fold(5, train_files)\n",
    "\n",
    "for i in range(5): \n",
    "    print(\"........ Fold {} Training ........\".format(i+1)) \n",
    "        \n",
    "    \n",
    "    # split data in train and validations et \n",
    "    td = train_folds[:i] + train_folds[i+1:] \n",
    "    train_data = [] \n",
    "    for j in td: \n",
    "        for name in j: \n",
    "            train_data.append(name)\n",
    "    val_data = train_folds[i] \n",
    "    \n",
    "    # create partition dictionary and parameter dictionary \n",
    "    partition = {'train':[], 'validation':[]} \n",
    "    params_train_gen = {'dim': (120,120),\n",
    "                    'batch_size': 32,\n",
    "                    'n_channels': 4,\n",
    "                    'n_timesteps': 4,\n",
    "                    'shuffle': True,\n",
    "                    'augment_data': True} \n",
    "\n",
    "    params_val_gen = {'dim': (120,120), \n",
    "                  'batch_size': 32, \n",
    "                  'n_channels': 4, \n",
    "                  'n_timesteps': 4,\n",
    "                  'shuffle': True,\n",
    "                  'augment_data': False}\n",
    "\n",
    "\n",
    "    for filename in train_data: \n",
    "        partition['train'].append(filename) \n",
    "    for filename in val_data: \n",
    "        partition['validation'].append(filename)  \n",
    "        \n",
    "    \n",
    "    # Generators\n",
    "    training_generator = DataGenerator(partition['train'], **params_train_gen)\n",
    "    validation_generator = DataGenerator(partition['validation'], **params_val_gen) \n",
    "    \n",
    "    # prepare model \n",
    "    model = base_UNET(64) \n",
    "\n",
    "    # conduct training \n",
    "    model_path = './storage/precip_unet_base/kfold' + str(i+1) + '/epoch_{epoch:03d}_val_loss_{val_loss:.3f}.h5'\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 3, verbose = 1, factor = 0.5)\n",
    "    checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10) \n",
    "    history = model.fit_generator(generator = training_generator, validation_data = validation_generator, epochs = 100, callbacks = [checkpoint, early_stopping, learning_rate_reduction])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
