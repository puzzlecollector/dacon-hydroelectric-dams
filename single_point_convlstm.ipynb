{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, BatchNormalization, concatenate, Input, ConvLSTM2D, Reshape, Conv3D, Flatten, LSTM, GRU, Dense,Dropout, Add\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(120,120), n_channels=1, n_timesteps = 4, shuffle=True, augment_data = True,\n",
    "                standardize = False):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_timesteps = n_timesteps \n",
    "        self.shuffle = shuffle\n",
    "        self.augment_data = augment_data  \n",
    "        self.standardize = standardize \n",
    "        self.on_epoch_end() \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        \n",
    "        if self.augment_data == True:  # only augment data when training \n",
    "            # Initialization\n",
    "            X = np.empty((self.batch_size*6, 120, 120, 4))\n",
    "            y = np.empty((self.batch_size*6, 120, 120, 1)) \n",
    "\n",
    "            # Generate data\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                data = np.load('./storage/precipitation/train/' + ID)\n",
    "                # Store sample\n",
    "                x_data = data[:,:,:4] \n",
    "                y_data = data[:,:,-1].reshape((120,120,1)) \n",
    "                \n",
    "                X[i,] = x_data\n",
    "                y[i] = y_data \n",
    "                \n",
    "                # add 90 degrees rotation \n",
    "                X[i+self.batch_size,] = np.rot90(x_data)\n",
    "                y[i+self.batch_size] = np.rot90(y_data)  \n",
    "                \n",
    "                # add 180 degrees rotation \n",
    "                X[i+self.batch_size*2,] = np.rot90(np.rot90(x_data)) \n",
    "                y[i+self.batch_size*2] = np.rot90(np.rot90(y_data)) \n",
    "                \n",
    "                # add 270 degrees rotation \n",
    "                X[i+self.batch_size*3,] = np.rot90(np.rot90(np.rot90(x_data)))\n",
    "                y[i+self.batch_size*3] = np.rot90(np.rot90(np.rot90(y_data)))  \n",
    "                \n",
    "                # add horizontal flip \n",
    "                X[i+self.batch_size*4,] = np.fliplr(x_data)\n",
    "                y[i+self.batch_size*4] = np.fliplr(y_data) \n",
    "                \n",
    "                # add vertical filp \n",
    "                X[i+self.batch_size*5,] = np.flipud(x_data) \n",
    "                y[i+self.batch_size*5] = np.flipud(y_data)\n",
    "            \n",
    "            # shuffle once more to make training harder \n",
    "            X,y = shuffle(X,y) \n",
    "            return (X, y)\n",
    "        \n",
    "        else: \n",
    "            # Initialization\n",
    "            x1 = [] \n",
    "            x2 = [] \n",
    "            y = [] \n",
    "\n",
    "            # Generate data\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                data = np.load('./storage/precipitation/train/' + ID).astype(np.float32) \n",
    "                data = np.log(data + 0.01) \n",
    "                \n",
    "                \n",
    "                if self.standardize:  \n",
    "                    data = (data - mu)/std\n",
    "                # for point(58,66)\n",
    "                x1.append(data[53:63,61:71,:4].reshape((4,10,10,1)))\n",
    "                x2.append(data[48:68,56:76,:4].reshape((4,20,20,1)))\n",
    "                y.append(data[53:63,61:71,-1].reshape((10,10,1))) \n",
    "            \n",
    "            x1 = np.asarray(x1)\n",
    "            x2 = np.asarray(x2)\n",
    "            y = np.asarray(y)\n",
    "            return [x1,x2],y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    inputs1 = Input((4,10,10,1))\n",
    "    inputs2 = Input((4,20,20,1)) \n",
    "    \n",
    "    convlstm1 = ConvLSTM2D(32,3,padding='same',return_sequences=True)(inputs1) \n",
    "    bn1 = BatchNormalization()(convlstm1) \n",
    "    convlstm1 = ConvLSTM2D(32,3,padding='same',return_sequences=False)(bn1)\n",
    "    bn1 = BatchNormalization()(convlstm1)\n",
    "    \n",
    "    convlstm2 = ConvLSTM2D(32,3,padding='same',return_sequences=True)(inputs2) \n",
    "    bn2 = BatchNormalization()(convlstm2) \n",
    "    maxpool = TimeDistributed(MaxPooling2D((2,2)))(bn2) \n",
    "    convlstm2 = ConvLSTM2D(32,3,padding='same',return_sequences=False)(maxpool) \n",
    "    bn2 = BatchNormalization()(convlstm2) \n",
    "    \n",
    "    concat = concatenate([bn1,bn2])  \n",
    "    outputs = Conv2D(1,1,padding='same',activation='relu')(concat) \n",
    "    model = Model(inputs=[inputs1,inputs2],outputs=outputs) \n",
    "    model.compile(loss='mae',optimizer='adam') \n",
    "    \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ Fold 1 Training ........\n",
      "Epoch 1/150\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 3.9226\n",
      "Epoch 00001: val_loss improved from inf to 3.87132, saving model to ./storage/convlstm_g3/kfold1/epoch_001_val_loss_3.871.h5\n",
      "1568/1568 [==============================] - 676s 431ms/step - loss: 3.9228 - val_loss: 3.8713\n",
      "Epoch 2/150\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 3.8777\n",
      "Epoch 00002: val_loss improved from 3.87132 to 3.85091, saving model to ./storage/convlstm_g3/kfold1/epoch_002_val_loss_3.851.h5\n",
      "1568/1568 [==============================] - 754s 481ms/step - loss: 3.8774 - val_loss: 3.8509\n",
      "Epoch 3/150\n",
      "1436/1568 [==========================>...] - ETA: 1:04 - loss: 3.8580"
     ]
    }
   ],
   "source": [
    "train_files = [x for x in os.listdir('./storage/precipitation/train/')] \n",
    "\n",
    "def k_fold(k,files):  \n",
    "    folds = [] \n",
    "    fold_size = len(files) // k \n",
    "    for i in range(k): \n",
    "        if i == k-1:  \n",
    "            l = files[i*fold_size:] \n",
    "        else: \n",
    "            l = files[i*fold_size:(i+1)*fold_size]  \n",
    "        folds.append(l)   \n",
    "    return folds  \n",
    "\n",
    "train_files = shuffle(train_files, random_state = 888) # shuffle train files before splitting them into a fold \n",
    "train_folds = k_fold(5, train_files)\n",
    "\n",
    "for i in range(5): \n",
    "    print(\"........ Fold {} Training ........\".format(i+1)) \n",
    "        \n",
    "    # split data in train and validations et \n",
    "    td = train_folds[:i] + train_folds[i+1:] \n",
    "    train_data = [] \n",
    "    for j in td: \n",
    "        for name in j: \n",
    "            train_data.append(name)\n",
    "    val_data = train_folds[i] \n",
    "    \n",
    "    # create partition dictionary and parameter dictionary \n",
    "    partition = {'train':[], 'validation':[]} \n",
    "    params_train_gen = {'dim': (120,120),\n",
    "                    'batch_size': 32,\n",
    "                    'n_channels': 4,\n",
    "                    'n_timesteps': 4,\n",
    "                    'shuffle': True,\n",
    "                    'augment_data': False,\n",
    "                    'standardize': False} \n",
    "\n",
    "    params_val_gen = {'dim': (120,120), \n",
    "                  'batch_size': 32, \n",
    "                  'n_channels': 4, \n",
    "                  'n_timesteps': 4,\n",
    "                  'shuffle': True,\n",
    "                  'augment_data': False,\n",
    "                  'standardize': False}\n",
    "\n",
    "\n",
    "    for filename in train_data: \n",
    "        partition['train'].append(filename) \n",
    "    for filename in val_data: \n",
    "        partition['validation'].append(filename)  \n",
    "        \n",
    "    \n",
    "    # Generators\n",
    "    training_generator = DataGenerator(partition['train'], **params_train_gen)\n",
    "    validation_generator = DataGenerator(partition['validation'], **params_val_gen) \n",
    "    \n",
    "    # prepare model \n",
    "    model = build_model() \n",
    "\n",
    "    # conduct training \n",
    "    model_path = './storage/convlstm_g3/kfold' + str(i+1) + '/epoch_{epoch:03d}_val_loss_{val_loss:.3f}.h5'\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 2, verbose = 1, factor = 0.8)\n",
    "    annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "    checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10) \n",
    "    history = model.fit_generator(generator = training_generator, validation_data = validation_generator, epochs = 150, callbacks = [checkpoint, early_stopping, learning_rate_reduction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
