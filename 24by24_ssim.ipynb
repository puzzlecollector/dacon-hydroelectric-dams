{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, BatchNormalization, concatenate, Input, ConvLSTM2D, Reshape, Conv3D, Flatten, LSTM, GRU, Dense,Dropout, Add\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and standard deviation for train data \n",
    "mu = 13.262550318358528\n",
    "std = 36.12859290913875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(120,120), n_channels=1, n_timesteps = 4, shuffle=True, augment_data = True,\n",
    "                standardize = False):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_timesteps = n_timesteps \n",
    "        self.shuffle = shuffle\n",
    "        self.augment_data = augment_data  \n",
    "        self.standardize = standardize \n",
    "        self.on_epoch_end() \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        # Initialization\n",
    "        X = [] \n",
    "        y = []\n",
    "\n",
    "            \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            data = np.load('./storage/precipitation/train/' + ID).astype(np.float32) \n",
    "            data /= 255.0 \n",
    "            if self.standardize==True:  \n",
    "                data = (data - self.mu)/self.std\n",
    "            for j in range(5): \n",
    "                for k in range(5): \n",
    "                    partial_x = data[24*j:24*(j+1),24*k:24*(k+1),:4] \n",
    "                    partial_y = data[24*j:24*(j+1),24*k:24*(k+1),-1] \n",
    "                    X.append(partial_x) \n",
    "                    y.append(partial_y)   \n",
    "        \n",
    "        X = np.asarray(X).reshape((-1,24,24,4))  \n",
    "        y = np.asarray(y).reshape((-1,24,24,1)) \n",
    "                \n",
    "        return X,y  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_loss(y_true, y_pred):\n",
    "    return 1-tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(start_neurons): \n",
    "    inputs = Input((24,24,4)) \n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    pool1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(pool1)\n",
    "\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    pool2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(pool2)\n",
    "\n",
    "    convm = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = BatchNormalization()(uconv1)\n",
    "    outputs = Conv2D(1, (1,1), padding=\"same\", activation=\"relu\")(uconv1)\n",
    "    model = Model(inputs=inputs,outputs=outputs) \n",
    "    model.compile(loss=ssim_loss,optimizer='adam') \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 24, 24, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 24, 24, 64)   2368        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 24, 24, 64)   256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 12, 12, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 12, 12, 128)  73856       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 12, 12, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 6, 6, 128)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 6, 6, 256)    295168      max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DTran (None, 12, 12, 128)  295040      conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 12, 12, 256)  0           conv2d_transpose_10[0][0]        \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 12, 12, 128)  295040      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 12, 12, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, 24, 24, 64)   73792       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 24, 24, 128)  0           conv2d_transpose_11[0][0]        \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 24, 24, 64)   73792       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 24, 24, 64)   256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 24, 24, 1)    65          batch_normalization_23[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 1,110,657\n",
      "Trainable params: 1,109,889\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_unet(64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.2447\n",
      "Epoch 00001: val_loss improved from inf to 0.24590, saving model to ./storage/precip_unet_test/epoch_001_val_loss_0.246.h5\n",
      "196/196 [==============================] - 483s 2s/step - loss: 0.2445 - val_loss: 0.2459\n",
      "Epoch 2/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1726\n",
      "Epoch 00002: val_loss improved from 0.24590 to 0.16305, saving model to ./storage/precip_unet_test/epoch_002_val_loss_0.163.h5\n",
      "196/196 [==============================] - 480s 2s/step - loss: 0.1725 - val_loss: 0.1630\n",
      "Epoch 3/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1297\n",
      "Epoch 00003: val_loss improved from 0.16305 to 0.12710, saving model to ./storage/precip_unet_test/epoch_003_val_loss_0.127.h5\n",
      "196/196 [==============================] - 476s 2s/step - loss: 0.1297 - val_loss: 0.1271\n",
      "Epoch 4/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1238\n",
      "Epoch 00004: val_loss improved from 0.12710 to 0.12416, saving model to ./storage/precip_unet_test/epoch_004_val_loss_0.124.h5\n",
      "196/196 [==============================] - 474s 2s/step - loss: 0.1238 - val_loss: 0.1242\n",
      "Epoch 5/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1223\n",
      "Epoch 00005: val_loss improved from 0.12416 to 0.12250, saving model to ./storage/precip_unet_test/epoch_005_val_loss_0.123.h5\n",
      "196/196 [==============================] - 478s 2s/step - loss: 0.1223 - val_loss: 0.1225\n",
      "Epoch 6/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1204\n",
      "Epoch 00006: val_loss improved from 0.12250 to 0.12096, saving model to ./storage/precip_unet_test/epoch_006_val_loss_0.121.h5\n",
      "196/196 [==============================] - 472s 2s/step - loss: 0.1204 - val_loss: 0.1210\n",
      "Epoch 7/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1184\n",
      "Epoch 00007: val_loss improved from 0.12096 to 0.11829, saving model to ./storage/precip_unet_test/epoch_007_val_loss_0.118.h5\n",
      "196/196 [==============================] - 485s 2s/step - loss: 0.1184 - val_loss: 0.1183\n",
      "Epoch 8/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1152\n",
      "Epoch 00008: val_loss improved from 0.11829 to 0.11541, saving model to ./storage/precip_unet_test/epoch_008_val_loss_0.115.h5\n",
      "196/196 [==============================] - 476s 2s/step - loss: 0.1151 - val_loss: 0.1154\n",
      "Epoch 9/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1130\n",
      "Epoch 00009: val_loss improved from 0.11541 to 0.11295, saving model to ./storage/precip_unet_test/epoch_009_val_loss_0.113.h5\n",
      "196/196 [==============================] - 479s 2s/step - loss: 0.1131 - val_loss: 0.1129\n",
      "Epoch 10/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1118\n",
      "Epoch 00010: val_loss improved from 0.11295 to 0.11135, saving model to ./storage/precip_unet_test/epoch_010_val_loss_0.111.h5\n",
      "196/196 [==============================] - 482s 2s/step - loss: 0.1118 - val_loss: 0.1114\n",
      "Epoch 11/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1101\n",
      "Epoch 00011: val_loss did not improve from 0.11135\n",
      "196/196 [==============================] - 491s 3s/step - loss: 0.1101 - val_loss: 0.1122\n",
      "Epoch 12/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1089\n",
      "Epoch 00012: val_loss improved from 0.11135 to 0.11054, saving model to ./storage/precip_unet_test/epoch_012_val_loss_0.111.h5\n",
      "196/196 [==============================] - 487s 2s/step - loss: 0.1089 - val_loss: 0.1105\n",
      "Epoch 13/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1076\n",
      "Epoch 00013: val_loss improved from 0.11054 to 0.10822, saving model to ./storage/precip_unet_test/epoch_013_val_loss_0.108.h5\n",
      "196/196 [==============================] - 487s 2s/step - loss: 0.1076 - val_loss: 0.1082\n",
      "Epoch 14/200\n",
      "195/196 [============================>.] - ETA: 1s - loss: 0.1057\n",
      "Epoch 00014: val_loss did not improve from 0.10822\n",
      "196/196 [==============================] - 375s 2s/step - loss: 0.1057 - val_loss: 0.1093\n",
      "Epoch 15/200\n",
      "195/196 [============================>.] - ETA: 1s - loss: 0.1045\n",
      "Epoch 00015: val_loss improved from 0.10822 to 0.10453, saving model to ./storage/precip_unet_test/epoch_015_val_loss_0.105.h5\n",
      "196/196 [==============================] - 460s 2s/step - loss: 0.1045 - val_loss: 0.1045\n",
      "Epoch 16/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1031\n",
      "Epoch 00016: val_loss did not improve from 0.10453\n",
      "196/196 [==============================] - 490s 3s/step - loss: 0.1031 - val_loss: 0.1060\n",
      "Epoch 17/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1020\n",
      "Epoch 00017: val_loss improved from 0.10453 to 0.10300, saving model to ./storage/precip_unet_test/epoch_017_val_loss_0.103.h5\n",
      "196/196 [==============================] - 486s 2s/step - loss: 0.1021 - val_loss: 0.1030\n",
      "Epoch 18/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1014\n",
      "Epoch 00018: val_loss improved from 0.10300 to 0.10229, saving model to ./storage/precip_unet_test/epoch_018_val_loss_0.102.h5\n",
      "196/196 [==============================] - 480s 2s/step - loss: 0.1014 - val_loss: 0.1023\n",
      "Epoch 19/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1007\n",
      "Epoch 00019: val_loss improved from 0.10229 to 0.10188, saving model to ./storage/precip_unet_test/epoch_019_val_loss_0.102.h5\n",
      "196/196 [==============================] - 488s 2s/step - loss: 0.1007 - val_loss: 0.1019\n",
      "Epoch 20/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.1000\n",
      "Epoch 00020: val_loss improved from 0.10188 to 0.10102, saving model to ./storage/precip_unet_test/epoch_020_val_loss_0.101.h5\n",
      "196/196 [==============================] - 471s 2s/step - loss: 0.1000 - val_loss: 0.1010\n",
      "Epoch 21/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0995\n",
      "Epoch 00021: val_loss improved from 0.10102 to 0.10065, saving model to ./storage/precip_unet_test/epoch_021_val_loss_0.101.h5\n",
      "196/196 [==============================] - 485s 2s/step - loss: 0.0995 - val_loss: 0.1007\n",
      "Epoch 22/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0991\n",
      "Epoch 00022: val_loss did not improve from 0.10065\n",
      "196/196 [==============================] - 478s 2s/step - loss: 0.0991 - val_loss: 0.1007\n",
      "Epoch 23/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0987\n",
      "Epoch 00023: val_loss improved from 0.10065 to 0.09973, saving model to ./storage/precip_unet_test/epoch_023_val_loss_0.100.h5\n",
      "196/196 [==============================] - 484s 2s/step - loss: 0.0987 - val_loss: 0.0997\n",
      "Epoch 24/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0984\n",
      "Epoch 00024: val_loss improved from 0.09973 to 0.09953, saving model to ./storage/precip_unet_test/epoch_024_val_loss_0.100.h5\n",
      "196/196 [==============================] - 479s 2s/step - loss: 0.0984 - val_loss: 0.0995\n",
      "Epoch 25/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0980\n",
      "Epoch 00025: val_loss did not improve from 0.09953\n",
      "196/196 [==============================] - 481s 2s/step - loss: 0.0980 - val_loss: 0.0998\n",
      "Epoch 26/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0976\n",
      "Epoch 00026: val_loss improved from 0.09953 to 0.09896, saving model to ./storage/precip_unet_test/epoch_026_val_loss_0.099.h5\n",
      "196/196 [==============================] - 486s 2s/step - loss: 0.0975 - val_loss: 0.0990\n",
      "Epoch 27/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0974\n",
      "Epoch 00027: val_loss improved from 0.09896 to 0.09832, saving model to ./storage/precip_unet_test/epoch_027_val_loss_0.098.h5\n",
      "196/196 [==============================] - 483s 2s/step - loss: 0.0974 - val_loss: 0.0983\n",
      "Epoch 28/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0969\n",
      "Epoch 00028: val_loss improved from 0.09832 to 0.09829, saving model to ./storage/precip_unet_test/epoch_028_val_loss_0.098.h5\n",
      "196/196 [==============================] - 483s 2s/step - loss: 0.0969 - val_loss: 0.0983\n",
      "Epoch 29/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0967\n",
      "Epoch 00029: val_loss did not improve from 0.09829\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "196/196 [==============================] - 478s 2s/step - loss: 0.0967 - val_loss: 0.0985\n",
      "Epoch 30/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0963\n",
      "Epoch 00030: val_loss improved from 0.09829 to 0.09786, saving model to ./storage/precip_unet_test/epoch_030_val_loss_0.098.h5\n",
      "196/196 [==============================] - 483s 2s/step - loss: 0.0963 - val_loss: 0.0979\n",
      "Epoch 31/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0961\n",
      "Epoch 00031: val_loss did not improve from 0.09786\n",
      "196/196 [==============================] - 487s 2s/step - loss: 0.0960 - val_loss: 0.0980\n",
      "Epoch 32/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0958\n",
      "Epoch 00032: val_loss improved from 0.09786 to 0.09726, saving model to ./storage/precip_unet_test/epoch_032_val_loss_0.097.h5\n",
      "196/196 [==============================] - 488s 2s/step - loss: 0.0958 - val_loss: 0.0973\n",
      "Epoch 33/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0956\n",
      "Epoch 00033: val_loss did not improve from 0.09726\n",
      "196/196 [==============================] - 477s 2s/step - loss: 0.0956 - val_loss: 0.0986\n",
      "Epoch 34/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0954\n",
      "Epoch 00034: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "196/196 [==============================] - 495s 3s/step - loss: 0.0954 - val_loss: 0.0976\n",
      "Epoch 35/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0950\n",
      "Epoch 00035: val_loss improved from 0.09726 to 0.09691, saving model to ./storage/precip_unet_test/epoch_035_val_loss_0.097.h5\n",
      "196/196 [==============================] - 488s 2s/step - loss: 0.0950 - val_loss: 0.0969\n",
      "Epoch 36/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0948\n",
      "Epoch 00036: val_loss did not improve from 0.09691\n",
      "196/196 [==============================] - 487s 2s/step - loss: 0.0948 - val_loss: 0.0971\n",
      "Epoch 37/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0947\n",
      "Epoch 00037: val_loss improved from 0.09691 to 0.09665, saving model to ./storage/precip_unet_test/epoch_037_val_loss_0.097.h5\n",
      "196/196 [==============================] - 487s 2s/step - loss: 0.0947 - val_loss: 0.0966\n",
      "Epoch 38/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0944\n",
      "Epoch 00038: val_loss did not improve from 0.09665\n",
      "196/196 [==============================] - 490s 3s/step - loss: 0.0944 - val_loss: 0.0967\n",
      "Epoch 39/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0943\n",
      "Epoch 00039: val_loss did not improve from 0.09665\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "196/196 [==============================] - 483s 2s/step - loss: 0.0943 - val_loss: 0.0968\n",
      "Epoch 40/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0939\n",
      "Epoch 00040: val_loss improved from 0.09665 to 0.09653, saving model to ./storage/precip_unet_test/epoch_040_val_loss_0.097.h5\n",
      "196/196 [==============================] - 485s 2s/step - loss: 0.0939 - val_loss: 0.0965\n",
      "Epoch 41/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0937\n",
      "Epoch 00041: val_loss did not improve from 0.09653\n",
      "196/196 [==============================] - 488s 2s/step - loss: 0.0937 - val_loss: 0.0979\n",
      "Epoch 42/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0935\n",
      "Epoch 00042: val_loss did not improve from 0.09653\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "196/196 [==============================] - 487s 2s/step - loss: 0.0935 - val_loss: 0.0975\n",
      "Epoch 43/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0932\n",
      "Epoch 00043: val_loss improved from 0.09653 to 0.09632, saving model to ./storage/precip_unet_test/epoch_043_val_loss_0.096.h5\n",
      "196/196 [==============================] - 542s 3s/step - loss: 0.0932 - val_loss: 0.0963\n",
      "Epoch 44/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0930\n",
      "Epoch 00044: val_loss did not improve from 0.09632\n",
      "196/196 [==============================] - 590s 3s/step - loss: 0.0930 - val_loss: 0.0969\n",
      "Epoch 45/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0929\n",
      "Epoch 00045: val_loss improved from 0.09632 to 0.09621, saving model to ./storage/precip_unet_test/epoch_045_val_loss_0.096.h5\n",
      "196/196 [==============================] - 493s 3s/step - loss: 0.0930 - val_loss: 0.0962\n",
      "Epoch 46/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0927\n",
      "Epoch 00046: val_loss did not improve from 0.09621\n",
      "196/196 [==============================] - 489s 2s/step - loss: 0.0927 - val_loss: 0.0968\n",
      "Epoch 47/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0926\n",
      "Epoch 00047: val_loss did not improve from 0.09621\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "196/196 [==============================] - 488s 2s/step - loss: 0.0926 - val_loss: 0.0966\n",
      "Epoch 48/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0922\n",
      "Epoch 00048: val_loss did not improve from 0.09621\n",
      "196/196 [==============================] - 483s 2s/step - loss: 0.0922 - val_loss: 0.0965\n",
      "Epoch 49/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0920\n",
      "Epoch 00049: val_loss did not improve from 0.09621\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "196/196 [==============================] - 485s 2s/step - loss: 0.0920 - val_loss: 0.0963\n",
      "Epoch 50/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0917\n",
      "Epoch 00050: val_loss did not improve from 0.09621\n",
      "196/196 [==============================] - 480s 2s/step - loss: 0.0917 - val_loss: 0.0964\n",
      "Epoch 51/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0916\n",
      "Epoch 00051: val_loss did not improve from 0.09621\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "196/196 [==============================] - 485s 2s/step - loss: 0.0916 - val_loss: 0.0963\n",
      "Epoch 52/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0913\n",
      "Epoch 00052: val_loss improved from 0.09621 to 0.09616, saving model to ./storage/precip_unet_test/epoch_052_val_loss_0.096.h5\n",
      "196/196 [==============================] - 485s 2s/step - loss: 0.0913 - val_loss: 0.0962\n",
      "Epoch 53/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0912\n",
      "Epoch 00053: val_loss did not improve from 0.09616\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "196/196 [==============================] - 480s 2s/step - loss: 0.0912 - val_loss: 0.0965\n",
      "Epoch 54/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0910\n",
      "Epoch 00054: val_loss did not improve from 0.09616\n",
      "196/196 [==============================] - 490s 3s/step - loss: 0.0909 - val_loss: 0.0966\n",
      "Epoch 55/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0909\n",
      "Epoch 00055: val_loss did not improve from 0.09616\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "196/196 [==============================] - 607s 3s/step - loss: 0.0908 - val_loss: 0.0963\n",
      "Epoch 56/200\n",
      "195/196 [============================>.] - ETA: 3s - loss: 0.0906\n",
      "Epoch 00056: val_loss did not improve from 0.09616\n",
      "196/196 [==============================] - 798s 4s/step - loss: 0.0906 - val_loss: 0.0963\n",
      "Epoch 57/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0905\n",
      "Epoch 00057: val_loss did not improve from 0.09616\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "196/196 [==============================] - 637s 3s/step - loss: 0.0905 - val_loss: 0.0965\n",
      "Epoch 58/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0904\n",
      "Epoch 00058: val_loss did not improve from 0.09616\n",
      "196/196 [==============================] - 582s 3s/step - loss: 0.0904 - val_loss: 0.0962\n",
      "Epoch 59/200\n",
      "195/196 [============================>.] - ETA: 3s - loss: 0.0903\n",
      "Epoch 00059: val_loss did not improve from 0.09616\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "196/196 [==============================] - 852s 4s/step - loss: 0.0903 - val_loss: 0.0964\n",
      "Epoch 60/200\n",
      "195/196 [============================>.] - ETA: 2s - loss: 0.0901\n",
      "Epoch 00060: val_loss did not improve from 0.09616\n",
      "196/196 [==============================] - 713s 4s/step - loss: 0.0901 - val_loss: 0.0965\n",
      "Epoch 61/200\n",
      "195/196 [============================>.] - ETA: 3s - loss: 0.0901\n",
      "Epoch 00061: val_loss did not improve from 0.09616\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "196/196 [==============================] - 793s 4s/step - loss: 0.0901 - val_loss: 0.0965\n",
      "Epoch 62/200\n",
      "195/196 [============================>.] - ETA: 3s - loss: 0.0899\n",
      "Epoch 00062: val_loss did not improve from 0.09616\n",
      "196/196 [==============================] - 803s 4s/step - loss: 0.0899 - val_loss: 0.0964\n"
     ]
    }
   ],
   "source": [
    "# due to time limitations, we will not do k-fold ensemble \n",
    "# fix the train and validation sets. \n",
    "train_files = [x for x in os.listdir('./storage/precipitation/train/')] \n",
    "train_files = shuffle(train_files)\n",
    "k = int(0.8 * len(train_files)) \n",
    "train_data = train_files[:k]\n",
    "val_data = train_files[k:]\n",
    "\n",
    "partition = {'train':[], 'validation':[]} \n",
    "\n",
    "for filename in train_data: \n",
    "    partition['train'].append(filename) \n",
    "for filename in val_data: \n",
    "    partition['validation'].append(filename)  \n",
    "\n",
    "cnt = 1 \n",
    "params_train_gen = {'dim': (120,120),\n",
    "                    'batch_size': 256,\n",
    "                    'n_channels': 4,\n",
    "                    'n_timesteps': 4,\n",
    "                    'shuffle': True,\n",
    "                    'augment_data': True}  \n",
    "\n",
    "params_val_gen = {'dim': (120,120), \n",
    "                  'batch_size': 256, \n",
    "                  'n_channels': 4, \n",
    "                  'n_timesteps': 4,\n",
    "                  'shuffle': True,\n",
    "                  'augment_data': False}  \n",
    "         \n",
    "\n",
    "training_generator = DataGenerator(partition['train'], **params_train_gen)\n",
    "validation_generator = DataGenerator(partition['validation'], **params_val_gen) \n",
    "model = build_unet(64)\n",
    "        \n",
    "model_path = './storage/precip_unet_test/epoch_{epoch:03d}_val_loss_{val_loss:.3f}.h5'\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 2, verbose = 1, factor = 0.8)\n",
    "checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10) \n",
    "history = model.fit_generator(generator = training_generator, validation_data = validation_generator, epochs = 200, callbacks = [checkpoint, early_stopping, learning_rate_reduction]) \n",
    "cnt += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for prediction \n",
    "test_path = './storage/precipitation/test'\n",
    "test_files = sorted(glob.glob(test_path + '/*.npy'))\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for file in tqdm(test_files, desc = 'test'):\n",
    "    data = np.load(file)\n",
    "    X_test.append(data)\n",
    "\n",
    "X_test = np.array(X_test).astype(np.float32)\n",
    "\n",
    "X_test /= 255.0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
