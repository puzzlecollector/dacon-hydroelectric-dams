{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, BatchNormalization, concatenate, Input, ConvLSTM2D, Reshape, Conv3D, Flatten, LSTM, GRU, Dense,Dropout, Add\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and standard deviation for train data \n",
    "mu = 13.262550318358528\n",
    "std = 36.12859290913875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(120,120), n_channels=1, n_timesteps = 4, shuffle=True, augment_data = True,\n",
    "                standardize = False):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_timesteps = n_timesteps \n",
    "        self.shuffle = shuffle\n",
    "        self.augment_data = augment_data  \n",
    "        self.standardize = standardize \n",
    "        self.on_epoch_end() \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        \n",
    "        if self.augment_data == True:  # only augment data when training - ignore this for now \n",
    "            # Initialization\n",
    "            X = np.empty((self.batch_size*6, 120, 120, 4))\n",
    "            y = np.empty((self.batch_size*6, 120, 120, 1)) \n",
    "\n",
    "            # Generate data\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                data = np.load('./storage/precipitation/train/' + ID)\n",
    "                # Store sample\n",
    "                x_data = data[:,:,:4] \n",
    "                y_data = data[:,:,-1].reshape((120,120,1)) \n",
    "                \n",
    "                X[i,] = x_data\n",
    "                y[i] = y_data \n",
    "                \n",
    "                # add 90 degrees rotation \n",
    "                X[i+self.batch_size,] = np.rot90(x_data)\n",
    "                y[i+self.batch_size] = np.rot90(y_data)  \n",
    "                \n",
    "                # add 180 degrees rotation \n",
    "                X[i+self.batch_size*2,] = np.rot90(np.rot90(x_data)) \n",
    "                y[i+self.batch_size*2] = np.rot90(np.rot90(y_data)) \n",
    "                \n",
    "                # add 270 degrees rotation \n",
    "                X[i+self.batch_size*3,] = np.rot90(np.rot90(np.rot90(x_data)))\n",
    "                y[i+self.batch_size*3] = np.rot90(np.rot90(np.rot90(y_data)))  \n",
    "                \n",
    "                # add horizontal flip \n",
    "                X[i+self.batch_size*4,] = np.fliplr(x_data)\n",
    "                y[i+self.batch_size*4] = np.fliplr(y_data) \n",
    "                \n",
    "                # add vertical filp \n",
    "                X[i+self.batch_size*5,] = np.flipud(x_data) \n",
    "                y[i+self.batch_size*5] = np.flipud(y_data)\n",
    "            \n",
    "            # shuffle once more to make training harder \n",
    "            X,y = shuffle(X,y) \n",
    "            return (X, y)\n",
    "        \n",
    "        else: \n",
    "            # Initialization\n",
    "            size = 20 \n",
    "            #X = np.empty((self.batch_size * 36, 4, size, size, 1))\n",
    "            #y = np.empty((self.batch_size * 36, size, size, 1)) \n",
    "\n",
    "            x_train = [] \n",
    "            y_train = [] \n",
    "            \n",
    "            # Generate data\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                data = np.load('./storage/precipitation/train/' + ID).astype(np.float32) \n",
    "                if self.standardize:  \n",
    "                    data = (data - mu)/std\n",
    "\n",
    "                for j in range(6): \n",
    "                    for k in range(6):  \n",
    "                        x_train.append(data[size*j:size*(j+1),size*k:size*(k+1),:4]) \n",
    "                        y_train.append(data[size*j:size*(j+1),size*k:size*(k+1),-1]) \n",
    "                        \n",
    "            x_train = np.asarray(x_train).reshape((-1,4,20,20,1)) \n",
    "            y_train = np.asarray(y_train).reshape((-1,20,20,1))\n",
    "        \n",
    "            return x_train, y_train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rnn(): \n",
    "    inputs = Input((4,20,20,1)) \n",
    "    bn = BatchNormalization()(inputs)\n",
    "    conv = ConvLSTM2D(32, 3, padding = 'same', return_sequences = True)(bn)\n",
    "    bn = BatchNormalization()(conv) \n",
    "    conv = ConvLSTM2D(32, 3, padding = 'same', return_sequences = False)(bn)\n",
    "    bn = BatchNormalization()(conv)  \n",
    "    outputs = Conv2D(1, 1, padding = \"same\", activation = 'relu')(bn) \n",
    "    model = Model(inputs=inputs,outputs=outputs) \n",
    "    model.compile(loss='mae',optimizer='adam')\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 4, 20, 20, 1)]    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 20, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_6 (ConvLSTM2D)  (None, 4, 20, 20, 32)     38144     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 20, 20, 32)     128       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_7 (ConvLSTM2D)  (None, 20, 20, 32)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 20, 20, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 1)         33        \n",
      "=================================================================\n",
      "Total params: 112,293\n",
      "Trainable params: 112,163\n",
      "Non-trainable params: 130\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simple_rnn() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 7.4555\n",
      "Epoch 00001: val_loss improved from inf to 6.53018, saving model to ./storage/precip_rnn_whole_2/epoch_001_val_loss_6.530.h5\n",
      "1568/1568 [==============================] - 1230s 784ms/step - loss: 7.4543 - val_loss: 6.5302\n",
      "Epoch 2/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 6.0614\n",
      "Epoch 00002: val_loss improved from 6.53018 to 5.78818, saving model to ./storage/precip_rnn_whole_2/epoch_002_val_loss_5.788.h5\n",
      "1568/1568 [==============================] - 1223s 780ms/step - loss: 6.0611 - val_loss: 5.7882\n",
      "Epoch 3/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.7970\n",
      "Epoch 00003: val_loss improved from 5.78818 to 5.69138, saving model to ./storage/precip_rnn_whole_2/epoch_003_val_loss_5.691.h5\n",
      "1568/1568 [==============================] - 1223s 780ms/step - loss: 5.7973 - val_loss: 5.6914\n",
      "Epoch 4/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.6642\n",
      "Epoch 00004: val_loss improved from 5.69138 to 5.50927, saving model to ./storage/precip_rnn_whole_2/epoch_004_val_loss_5.509.h5\n",
      "1568/1568 [==============================] - 1223s 780ms/step - loss: 5.6632 - val_loss: 5.5093\n",
      "Epoch 5/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.5900\n",
      "Epoch 00005: val_loss improved from 5.50927 to 5.47553, saving model to ./storage/precip_rnn_whole_2/epoch_005_val_loss_5.476.h5\n",
      "1568/1568 [==============================] - 1224s 781ms/step - loss: 5.5897 - val_loss: 5.4755\n",
      "Epoch 6/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.5382\n",
      "Epoch 00006: val_loss improved from 5.47553 to 5.42611, saving model to ./storage/precip_rnn_whole_2/epoch_006_val_loss_5.426.h5\n",
      "1568/1568 [==============================] - 1235s 787ms/step - loss: 5.5379 - val_loss: 5.4261\n",
      "Epoch 7/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.5063\n",
      "Epoch 00007: val_loss did not improve from 5.42611\n",
      "1568/1568 [==============================] - 1233s 786ms/step - loss: 5.5056 - val_loss: 5.4594\n",
      "Epoch 8/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.4832\n",
      "Epoch 00008: val_loss did not improve from 5.42611\n",
      "1568/1568 [==============================] - 1233s 786ms/step - loss: 5.4830 - val_loss: 5.4549\n",
      "Epoch 9/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.4502\n",
      "Epoch 00009: val_loss improved from 5.42611 to 5.41448, saving model to ./storage/precip_rnn_whole_2/epoch_009_val_loss_5.414.h5\n",
      "1568/1568 [==============================] - 1232s 786ms/step - loss: 5.4502 - val_loss: 5.4145\n",
      "Epoch 10/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.4366\n",
      "Epoch 00010: val_loss did not improve from 5.41448\n",
      "1568/1568 [==============================] - 1232s 786ms/step - loss: 5.4360 - val_loss: 5.4282\n",
      "Epoch 11/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.4195\n",
      "Epoch 00011: val_loss improved from 5.41448 to 5.31883, saving model to ./storage/precip_rnn_whole_2/epoch_011_val_loss_5.319.h5\n",
      "1568/1568 [==============================] - 1231s 785ms/step - loss: 5.4191 - val_loss: 5.3188\n",
      "Epoch 12/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.4085\n",
      "Epoch 00012: val_loss did not improve from 5.31883\n",
      "1568/1568 [==============================] - 1232s 786ms/step - loss: 5.4086 - val_loss: 5.4086\n",
      "Epoch 13/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3982\n",
      "Epoch 00013: val_loss improved from 5.31883 to 5.30843, saving model to ./storage/precip_rnn_whole_2/epoch_013_val_loss_5.308.h5\n",
      "1568/1568 [==============================] - 1233s 786ms/step - loss: 5.3981 - val_loss: 5.3084\n",
      "Epoch 14/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3850\n",
      "Epoch 00014: val_loss did not improve from 5.30843\n",
      "1568/1568 [==============================] - 1232s 786ms/step - loss: 5.3850 - val_loss: 5.3267\n",
      "Epoch 15/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3852\n",
      "Epoch 00015: val_loss did not improve from 5.30843\n",
      "1568/1568 [==============================] - 1233s 786ms/step - loss: 5.3854 - val_loss: 5.3366\n",
      "Epoch 16/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3684\n",
      "Epoch 00016: val_loss did not improve from 5.30843\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "1568/1568 [==============================] - 1232s 786ms/step - loss: 5.3679 - val_loss: 5.3484\n",
      "Epoch 17/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3484\n",
      "Epoch 00017: val_loss improved from 5.30843 to 5.27837, saving model to ./storage/precip_rnn_whole_2/epoch_017_val_loss_5.278.h5\n",
      "1568/1568 [==============================] - 1233s 786ms/step - loss: 5.3481 - val_loss: 5.2784\n",
      "Epoch 18/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3403\n",
      "Epoch 00018: val_loss improved from 5.27837 to 5.26169, saving model to ./storage/precip_rnn_whole_2/epoch_018_val_loss_5.262.h5\n",
      "1568/1568 [==============================] - 1234s 787ms/step - loss: 5.3410 - val_loss: 5.2617\n",
      "Epoch 19/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3373\n",
      "Epoch 00019: val_loss did not improve from 5.26169\n",
      "1568/1568 [==============================] - 1232s 786ms/step - loss: 5.3373 - val_loss: 5.2860\n",
      "Epoch 20/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3307\n",
      "Epoch 00020: val_loss improved from 5.26169 to 5.25552, saving model to ./storage/precip_rnn_whole_2/epoch_020_val_loss_5.256.h5\n",
      "1568/1568 [==============================] - 1234s 787ms/step - loss: 5.3306 - val_loss: 5.2555\n",
      "Epoch 21/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3263\n",
      "Epoch 00021: val_loss did not improve from 5.25552\n",
      "1568/1568 [==============================] - 1234s 787ms/step - loss: 5.3260 - val_loss: 5.3325\n",
      "Epoch 22/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3200\n",
      "Epoch 00022: val_loss did not improve from 5.25552\n",
      "1568/1568 [==============================] - 1234s 787ms/step - loss: 5.3198 - val_loss: 5.3297\n",
      "Epoch 23/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3165\n",
      "Epoch 00023: val_loss did not improve from 5.25552\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "1568/1568 [==============================] - 1232s 786ms/step - loss: 5.3171 - val_loss: 5.2574\n",
      "Epoch 24/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3050\n",
      "Epoch 00024: val_loss improved from 5.25552 to 5.24605, saving model to ./storage/precip_rnn_whole_2/epoch_024_val_loss_5.246.h5\n",
      "1568/1568 [==============================] - 1234s 787ms/step - loss: 5.3047 - val_loss: 5.2460\n",
      "Epoch 25/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3000\n",
      "Epoch 00025: val_loss improved from 5.24605 to 5.24000, saving model to ./storage/precip_rnn_whole_2/epoch_025_val_loss_5.240.h5\n",
      "1568/1568 [==============================] - 1234s 787ms/step - loss: 5.2994 - val_loss: 5.2400\n",
      "Epoch 26/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.3018\n",
      "Epoch 00026: val_loss did not improve from 5.24000\n",
      "1568/1568 [==============================] - 1233s 786ms/step - loss: 5.3017 - val_loss: 5.2537\n",
      "Epoch 27/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.2950\n",
      "Epoch 00027: val_loss did not improve from 5.24000\n",
      "1568/1568 [==============================] - 1233s 786ms/step - loss: 5.2961 - val_loss: 5.2766\n",
      "Epoch 28/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.2981\n",
      "Epoch 00028: val_loss improved from 5.24000 to 5.23736, saving model to ./storage/precip_rnn_whole_2/epoch_028_val_loss_5.237.h5\n",
      "1568/1568 [==============================] - 1234s 787ms/step - loss: 5.2970 - val_loss: 5.2374\n",
      "Epoch 29/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.2896\n",
      "Epoch 00029: val_loss did not improve from 5.23736\n",
      "1568/1568 [==============================] - 1234s 787ms/step - loss: 5.2888 - val_loss: 5.2398\n",
      "Epoch 30/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.2894\n",
      "Epoch 00030: val_loss did not improve from 5.23736\n",
      "1568/1568 [==============================] - 1234s 787ms/step - loss: 5.2894 - val_loss: 5.2495\n",
      "Epoch 31/300\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 5.2814\n",
      "Epoch 00031: val_loss improved from 5.23736 to 5.22902, saving model to ./storage/precip_rnn_whole_2/epoch_031_val_loss_5.229.h5\n",
      "1568/1568 [==============================] - 1234s 787ms/step - loss: 5.2813 - val_loss: 5.2290\n",
      "Epoch 32/300\n",
      "1087/1568 [===================>..........] - ETA: 5:45 - loss: 5.2588"
     ]
    }
   ],
   "source": [
    "# due to time limitations, we will not do k-fold ensemble \n",
    "# fix the train and validation sets. \n",
    "train_files = [x for x in os.listdir('./storage/precipitation/train/')] \n",
    "train_files = shuffle(train_files)\n",
    "k = int(0.8 * len(train_files)) \n",
    "train_data = train_files[:k]\n",
    "val_data = train_files[k:]\n",
    "\n",
    "partition = {'train':[], 'validation':[]} \n",
    "\n",
    "for filename in train_data: \n",
    "    partition['train'].append(filename) \n",
    "for filename in val_data: \n",
    "    partition['validation'].append(filename)  \n",
    "\n",
    "params_train_gen = {'dim': (120,120),\n",
    "                    'batch_size': 32,\n",
    "                    'n_channels': 4,\n",
    "                    'n_timesteps': 4,\n",
    "                    'shuffle': True,\n",
    "                    'augment_data': False}  \n",
    "\n",
    "params_val_gen = {'dim': (120,120), \n",
    "                  'batch_size': 32, \n",
    "                  'n_channels': 4, \n",
    "                  'n_timesteps': 4,\n",
    "                  'shuffle': True,\n",
    "                  'augment_data': False} \n",
    "         \n",
    "\n",
    "training_generator = DataGenerator(partition['train'], **params_train_gen)\n",
    "validation_generator = DataGenerator(partition['validation'], **params_val_gen) \n",
    "model = simple_rnn()\n",
    "        \n",
    "model_path = './storage/precip_rnn_whole_2/epoch_{epoch:03d}_val_loss_{val_loss:.3f}.h5'\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 3, verbose = 1, factor = 0.8)\n",
    "checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 16) \n",
    "history = model.fit_generator(generator = training_generator, validation_data = validation_generator, epochs = 300, callbacks = [checkpoint, early_stopping, learning_rate_reduction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
