{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from tensorflow import keras\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model \n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, concatenate, Conv2D, Conv2DTranspose, Dropout, AlphaDropout, MaxPooling2D, AveragePooling2D, BatchNormalization, Concatenate, Flatten, Reshape, Add, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(120,120), n_channels=4, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, *self.dim, 1))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            data = np.load('./storage/precipitation/train/' + ID) \n",
    "            # Store sample\n",
    "            X[i,] = data[:,:,:4]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = data[:,:,-1].reshape((120,120,1)) \n",
    "        \n",
    "        return (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (120,120),\n",
    "          'batch_size': 32,\n",
    "          'n_channels': 4,\n",
    "          'shuffle': True}\n",
    "\n",
    "partition = {'train':[], 'validation':[]} \n",
    "train_files = [x for x in os.listdir('./storage/precipitation/train')] \n",
    "k = int(len(train_files) * 0.8) \n",
    "for i in range(0,k): \n",
    "    partition['train'].append(train_files[i]) \n",
    "for i in range(k,len(train_files)): \n",
    "    partition['validation'].append(train_files[i]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], **params)\n",
    "validation_generator = DataGenerator(partition['validation'], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model(input_layer, start_neurons):\n",
    "    \n",
    "    bn = BatchNormalization()(input_layer)\n",
    "    \n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(bn)\n",
    "    pool1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(pool1)\n",
    "\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    pool2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(pool2)\n",
    "\n",
    "    convm = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = BatchNormalization()(uconv1)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation='relu')(uconv1)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "input_layer = Input((120, 120, 4))\n",
    "output_layer = base_model(input_layer,64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 120, 120, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 120, 120, 4)  16          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 120, 120, 64) 2368        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 120, 120, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 60, 60, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 60, 60, 128)  73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 60, 60, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 30, 30, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 30, 30, 256)  295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 60, 60, 128)  295040      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60, 60, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 60, 60, 128)  295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 60, 60, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 120, 120, 64) 73792       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 120, 120, 128 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 120, 120, 64) 73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 120, 120, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 120, 120, 1)  65          batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,110,673\n",
      "Trainable params: 1,109,897\n",
      "Non-trainable params: 776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_layer, output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 4.0954\n",
      "Epoch 00001: val_loss improved from inf to 3.31308, saving model to ./storage/precipitation_best/epoch_001_val_3.313.h5\n",
      "1568/1568 [==============================] - 274s 175ms/step - loss: 4.0946 - val_loss: 3.3131\n",
      "Epoch 2/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 3.1123\n",
      "Epoch 00002: val_loss improved from 3.31308 to 2.97901, saving model to ./storage/precipitation_best/epoch_002_val_2.979.h5\n",
      "1568/1568 [==============================] - 273s 174ms/step - loss: 3.1123 - val_loss: 2.9790\n",
      "Epoch 3/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 3.0174\n",
      "Epoch 00003: val_loss did not improve from 2.97901\n",
      "1568/1568 [==============================] - 273s 174ms/step - loss: 3.0171 - val_loss: 2.9919\n",
      "Epoch 4/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.9753\n",
      "Epoch 00004: val_loss improved from 2.97901 to 2.93728, saving model to ./storage/precipitation_best/epoch_004_val_2.937.h5\n",
      "1568/1568 [==============================] - 276s 176ms/step - loss: 2.9755 - val_loss: 2.9373\n",
      "Epoch 5/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.9436\n",
      "Epoch 00005: val_loss improved from 2.93728 to 2.91145, saving model to ./storage/precipitation_best/epoch_005_val_2.911.h5\n",
      "1568/1568 [==============================] - 274s 174ms/step - loss: 2.9435 - val_loss: 2.9115\n",
      "Epoch 6/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.9271\n",
      "Epoch 00006: val_loss improved from 2.91145 to 2.87744, saving model to ./storage/precipitation_best/epoch_006_val_2.877.h5\n",
      "1568/1568 [==============================] - 274s 175ms/step - loss: 2.9268 - val_loss: 2.8774\n",
      "Epoch 7/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.9081\n",
      "Epoch 00007: val_loss did not improve from 2.87744\n",
      "1568/1568 [==============================] - 274s 175ms/step - loss: 2.9080 - val_loss: 2.8943\n",
      "Epoch 8/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8969\n",
      "Epoch 00008: val_loss improved from 2.87744 to 2.86083, saving model to ./storage/precipitation_best/epoch_008_val_2.861.h5\n",
      "1568/1568 [==============================] - 276s 176ms/step - loss: 2.8966 - val_loss: 2.8608\n",
      "Epoch 9/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8837\n",
      "Epoch 00009: val_loss did not improve from 2.86083\n",
      "1568/1568 [==============================] - 275s 175ms/step - loss: 2.8836 - val_loss: 2.8655\n",
      "Epoch 10/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8746\n",
      "Epoch 00010: val_loss did not improve from 2.86083\n",
      "1568/1568 [==============================] - 273s 174ms/step - loss: 2.8743 - val_loss: 2.8992\n",
      "Epoch 11/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8649\n",
      "Epoch 00011: val_loss did not improve from 2.86083\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "1568/1568 [==============================] - 283s 181ms/step - loss: 2.8652 - val_loss: 2.9259\n",
      "Epoch 12/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8428\n",
      "Epoch 00012: val_loss did not improve from 2.86083\n",
      "1568/1568 [==============================] - 288s 183ms/step - loss: 2.8425 - val_loss: 2.8911\n",
      "Epoch 13/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8342\n",
      "Epoch 00013: val_loss did not improve from 2.86083\n",
      "1568/1568 [==============================] - 287s 183ms/step - loss: 2.8344 - val_loss: 2.9161\n",
      "Epoch 14/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8339\n",
      "Epoch 00014: val_loss improved from 2.86083 to 2.85648, saving model to ./storage/precipitation_best/epoch_014_val_2.856.h5\n",
      "1568/1568 [==============================] - 285s 182ms/step - loss: 2.8334 - val_loss: 2.8565\n",
      "Epoch 15/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8284\n",
      "Epoch 00015: val_loss improved from 2.85648 to 2.85077, saving model to ./storage/precipitation_best/epoch_015_val_2.851.h5\n",
      "1568/1568 [==============================] - 284s 181ms/step - loss: 2.8289 - val_loss: 2.8508\n",
      "Epoch 16/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8245\n",
      "Epoch 00016: val_loss did not improve from 2.85077\n",
      "1568/1568 [==============================] - 283s 181ms/step - loss: 2.8246 - val_loss: 2.8978\n",
      "Epoch 17/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8188\n",
      "Epoch 00017: val_loss did not improve from 2.85077\n",
      "1568/1568 [==============================] - 289s 184ms/step - loss: 2.8186 - val_loss: 2.8728\n",
      "Epoch 18/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8151\n",
      "Epoch 00018: val_loss did not improve from 2.85077\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "1568/1568 [==============================] - 287s 183ms/step - loss: 2.8152 - val_loss: 2.8558\n",
      "Epoch 19/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.8008\n",
      "Epoch 00019: val_loss did not improve from 2.85077\n",
      "1568/1568 [==============================] - 287s 183ms/step - loss: 2.8009 - val_loss: 2.8544\n",
      "Epoch 20/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7950\n",
      "Epoch 00020: val_loss did not improve from 2.85077\n",
      "1568/1568 [==============================] - 278s 177ms/step - loss: 2.7950 - val_loss: 2.8737\n",
      "Epoch 21/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7932\n",
      "Epoch 00021: val_loss did not improve from 2.85077\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "1568/1568 [==============================] - 277s 176ms/step - loss: 2.7934 - val_loss: 2.8855\n",
      "Epoch 22/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7806\n",
      "Epoch 00022: val_loss did not improve from 2.85077\n",
      "1568/1568 [==============================] - 275s 175ms/step - loss: 2.7806 - val_loss: 2.8648\n",
      "Epoch 23/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7787\n",
      "Epoch 00023: val_loss did not improve from 2.85077\n",
      "1568/1568 [==============================] - 276s 176ms/step - loss: 2.7786 - val_loss: 2.8554\n",
      "Epoch 24/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7761\n",
      "Epoch 00024: val_loss did not improve from 2.85077\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "1568/1568 [==============================] - 275s 176ms/step - loss: 2.7763 - val_loss: 2.8618\n",
      "Epoch 25/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7659\n",
      "Epoch 00025: val_loss improved from 2.85077 to 2.82633, saving model to ./storage/precipitation_best/epoch_025_val_2.826.h5\n",
      "1568/1568 [==============================] - 280s 179ms/step - loss: 2.7658 - val_loss: 2.8263\n",
      "Epoch 26/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7662\n",
      "Epoch 00026: val_loss did not improve from 2.82633\n",
      "1568/1568 [==============================] - 282s 180ms/step - loss: 2.7661 - val_loss: 2.8300\n",
      "Epoch 27/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7598\n",
      "Epoch 00027: val_loss did not improve from 2.82633\n",
      "1568/1568 [==============================] - 275s 176ms/step - loss: 2.7595 - val_loss: 2.8392\n",
      "Epoch 28/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7612\n",
      "Epoch 00028: val_loss did not improve from 2.82633\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "1568/1568 [==============================] - 279s 178ms/step - loss: 2.7611 - val_loss: 2.8459\n",
      "Epoch 29/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7532\n",
      "Epoch 00029: val_loss did not improve from 2.82633\n",
      "1568/1568 [==============================] - 273s 174ms/step - loss: 2.7531 - val_loss: 2.8693\n",
      "Epoch 30/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7483\n",
      "Epoch 00030: val_loss did not improve from 2.82633\n",
      "1568/1568 [==============================] - 275s 175ms/step - loss: 2.7485 - val_loss: 2.8392\n",
      "Epoch 31/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7469\n",
      "Epoch 00031: val_loss did not improve from 2.82633\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "1568/1568 [==============================] - 274s 175ms/step - loss: 2.7476 - val_loss: 2.8803\n",
      "Epoch 32/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7430\n",
      "Epoch 00032: val_loss did not improve from 2.82633\n",
      "1568/1568 [==============================] - 276s 176ms/step - loss: 2.7429 - val_loss: 2.8525\n",
      "Epoch 33/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7397\n",
      "Epoch 00033: val_loss did not improve from 2.82633\n",
      "1568/1568 [==============================] - 274s 175ms/step - loss: 2.7396 - val_loss: 2.8619\n",
      "Epoch 34/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7394\n",
      "Epoch 00034: val_loss did not improve from 2.82633\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "1568/1568 [==============================] - 274s 175ms/step - loss: 2.7399 - val_loss: 2.8372\n",
      "Epoch 35/100\n",
      "1567/1568 [============================>.] - ETA: 0s - loss: 2.7325\n",
      "Epoch 00035: val_loss did not improve from 2.82633\n",
      "1568/1568 [==============================] - 276s 176ms/step - loss: 2.7325 - val_loss: 2.8290\n"
     ]
    }
   ],
   "source": [
    "model_path = './storage/precipitation_best/epoch_{epoch:03d}_val_{val_loss:.3f}.h5' \n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.8)\n",
    "checkpoint = ModelCheckpoint(filepath=model_path,monitor='val_loss',verbose=1,save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=10) \n",
    "\n",
    "model.compile(optimize = 'adam', loss = 'mae')\n",
    "history = model.fit_generator(generator = training_generator, validation_data = validation_generator, epochs = 100, callbacks=[checkpoint, early_stopping, learning_rate_reduction])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 2674/2674 [00:03<00:00, 767.57it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = './storage/precipitation/test'\n",
    "test_files = sorted(glob.glob(test_path + '/*.npy'))\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for file in tqdm(test_files, desc = 'test'):\n",
    "    data = np.load(file)\n",
    "    X_test.append(data)\n",
    "\n",
    "X_test = np.array(X_test).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('./storage/precipitation_best/epoch_025_val_2.826.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 120, 120, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 120, 120, 4)  16          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 120, 120, 64) 2368        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 120, 120, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 60, 60, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 60, 60, 128)  73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 60, 60, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 30, 30, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 30, 30, 256)  295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 60, 60, 128)  295040      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60, 60, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 60, 60, 128)  295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 60, 60, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 120, 120, 64) 73792       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 120, 120, 128 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 120, 120, 64) 73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 120, 120, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 120, 120, 1)  65          batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,110,673\n",
      "Trainable params: 1,109,897\n",
      "Non-trainable params: 776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./storage/precipitation/sample_submission.csv') \n",
    "submission.iloc[:,1:] = predictions.reshape(-1, 14400).astype(int)\n",
    "submission.to_csv('./storage/unet_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>14390</th>\n",
       "      <th>14391</th>\n",
       "      <th>14392</th>\n",
       "      <th>14393</th>\n",
       "      <th>14394</th>\n",
       "      <th>14395</th>\n",
       "      <th>14396</th>\n",
       "      <th>14397</th>\n",
       "      <th>14398</th>\n",
       "      <th>14399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_00000.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_00001.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_00002.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_00003.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_00004.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_name  0  1  2  3  4  5  6  7  8  ...  14390  14391  14392  14393  \\\n",
       "0  test_00000.npy  0  0  0  0  0  0  0  0  0  ...      0      0      0      0   \n",
       "1  test_00001.npy  0  0  0  0  0  0  0  0  0  ...      0      0      0      0   \n",
       "2  test_00002.npy  0  0  0  0  0  0  0  0  0  ...      0      0      0      0   \n",
       "3  test_00003.npy  0  0  0  0  0  0  0  0  0  ...      0      0      0      0   \n",
       "4  test_00004.npy  0  0  0  0  0  0  0  0  0  ...      0      0      0      0   \n",
       "\n",
       "   14394  14395  14396  14397  14398  14399  \n",
       "0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 14401 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
